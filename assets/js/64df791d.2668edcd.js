"use strict";(self.webpackChunkmsamp_website=self.webpackChunkmsamp_website||[]).push([[498],{7620:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return p},default:function(){return m},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return l}});var n=a(7462),i=a(3366),r=(a(7294),a(3905)),o=["components"],s={id:"usage"},p="Use MS-AMP",d={unversionedId:"user-tutorial/usage",id:"user-tutorial/usage",isDocsHomePage:!1,title:"Use MS-AMP",description:"Basic usage",source:"@site/../docs/user-tutorial/usage.md",sourceDirName:"user-tutorial",slug:"/user-tutorial/usage",permalink:"/MS-AMP/docs/user-tutorial/usage",editUrl:"https://github.com/azure/MS-AMP/edit/main/website/../docs/user-tutorial/usage.md",version:"current",frontMatter:{id:"usage"},sidebar:"docs",previous:{title:"Run Examples",permalink:"/MS-AMP/docs/getting-started/run-msamp"},next:{title:"Optimization Level",permalink:"/MS-AMP/docs/user-tutorial/optimization-level"}},l=[{value:"Basic usage",id:"basic-usage",children:[]},{value:"Usage in DeepSpeed",id:"usage-in-deepspeed",children:[]},{value:"Usage in Megatron-DeepSpeed and Megatron-LM",id:"usage-in-megatron-deepspeed-and-megatron-lm",children:[]}],u={toc:l};function m(e){var t=e.components,a=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"use-ms-amp"},"Use MS-AMP"),(0,r.kt)("h2",{id:"basic-usage"},"Basic usage"),(0,r.kt)("p",null,"Enabling MS-AMP is very simple when traning model w/o any distributed parallel technologies, you only need to add one line of code ",(0,r.kt)("inlineCode",{parentName:"p"},"msamp.initialize(model, optimizer, opt_level)")," after defining model and optimizer."),(0,r.kt)("p",null,"Example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import msamp\n\n# Declare model and optimizer as usual, with default (FP32) precision\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Allow MS-AMP to perform casts as required by the opt_level\nmodel, optimizer = msamp.initialize(model, optimizer, opt_level="O2")\n...\n')),(0,r.kt)("h2",{id:"usage-in-deepspeed"},"Usage in DeepSpeed"),(0,r.kt)("p",null,"MS-AMP supports FP8 for distributed parallel training and has the capability of integrating with advanced distributed traning frameworks. We have integrated MS-AMP with several popular distributed training frameworks such as DeepSpeed, Megatron-DeepSpeed and Megatron-LM to demonstrate this capability."),(0,r.kt)("p",null,"For enabling MS-AMP in DeepSpeed, add one line of code ",(0,r.kt)("inlineCode",{parentName:"p"},"from msamp import deepspeed"),' at the beginging and a "msamp" section in DeepSpeed config file:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'"msamp": {\n  "enabled": true,\n  "opt_level": "O1|O2|O3"\n}\n')),(0,r.kt)("p",null,'"O3" is designed for FP8 in ZeRO optimizer, so please make sure ZeRO is enabled when using "O3".'),(0,r.kt)("h2",{id:"usage-in-megatron-deepspeed-and-megatron-lm"},"Usage in Megatron-DeepSpeed and Megatron-LM"),(0,r.kt)("p",null,"For integrating MS-AMP with Megatron-DeepSpeed and Megatron-LM, you need to make some code changes. We provide a patch as a reference for the integration. Here is the instruction of integrating MS-AMP with Megatron-DeepSpeed/Megatron-LM and how to run ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Azure/MS-AMP-Examples/tree/main/gpt3"},"gpt-3")," with MS-AMP."),(0,r.kt)("p",null,"Runnable, simple examples demonstrating good practices can be found ",(0,r.kt)("a",{parentName:"p",href:"https://azure.github.io//MS-AMP/docs/getting-started/run-msamp"},"here"),".\nFor more comprehensive examples, please go to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Azure/MS-AMP-Examples"},"MS-AMP-Examples"),"."))}m.isMDXComponent=!0}}]);
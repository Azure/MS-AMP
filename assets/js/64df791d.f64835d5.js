"use strict";(self.webpackChunkmsamp_website=self.webpackChunkmsamp_website||[]).push([[498],{7620:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return u}});var n=a(7462),i=a(3366),r=(a(7294),a(3905)),o=["components"],s={id:"usage"},l="Use MS-AMP",p={unversionedId:"user-tutorial/usage",id:"user-tutorial/usage",isDocsHomePage:!1,title:"Use MS-AMP",description:"Basic usage",source:"@site/../docs/user-tutorial/usage.md",sourceDirName:"user-tutorial",slug:"/user-tutorial/usage",permalink:"/MS-AMP/docs/user-tutorial/usage",editUrl:"https://github.com/azure/MS-AMP/edit/main/website/../docs/user-tutorial/usage.md",version:"current",frontMatter:{id:"usage"},sidebar:"docs",previous:{title:"Run examples",permalink:"/MS-AMP/docs/getting-started/run-msamp"},next:{title:"optimization-level",permalink:"/MS-AMP/docs/user-tutorial/optimization-level"}},u=[{value:"Basic usage",id:"basic-usage",children:[]},{value:"Usage in distributed parallel training",id:"usage-in-distributed-parallel-training",children:[]}],d={toc:u};function m(e){var t=e.components,a=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"use-ms-amp"},"Use MS-AMP"),(0,r.kt)("h2",{id:"basic-usage"},"Basic usage"),(0,r.kt)("p",null,"Enabling MS-AMP is very simple when traning model w/ or w/o data parallelism on a single node, you only need to add one line of code ",(0,r.kt)("inlineCode",{parentName:"p"},"msamp.initialize(model, optimizer, opt_level)")," after defining model and optimizer."),(0,r.kt)("p",null,"Example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import msamp\n\n# Declare model and optimizer as usual, with default (FP32) precision\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Allow MS-AMP to perform casts as required by the opt_level\nmodel, optimizer = msamp.initialize(model, optimizer, opt_level="O2")\n...\n')),(0,r.kt)("h2",{id:"usage-in-distributed-parallel-training"},"Usage in distributed parallel training"),(0,r.kt)("p",null,"MS-AMP supports FP8 for distributed parallel training and has the capability of integrating with advanced distributed traning frameworks. We have integrated MS-AMP with several popular distributed training frameworks such as DeepSpeed, Megatron-DeepSpeed and Megatron-LM to demonstrate this capability."),(0,r.kt)("p",null,"For enabling MS-AMP when using ZeRO in DeepSpeed, add one line of code ",(0,r.kt)("inlineCode",{parentName:"p"},"import msamp"),' and a "msamp" section in DeepSpeed config file:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'"msamp": {\n  "enabled": true,\n  "opt_level": "O3"\n}\n')),(0,r.kt)("p",null,"For applying MS-AMP to Megatron-DeepSpeed and Megatron-LM, you need to do very little code change for applying it. Here is the instruction of applying MS-AMP for running ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Azure/MS-AMP-Examples/tree/main/gpt3"},"gpt-3")," in both Megatron-DeepSpeed and Megatron-LM."),(0,r.kt)("p",null,"Runnable, simple examples demonstrating good practices can be found ",(0,r.kt)("a",{parentName:"p",href:"https://azure.github.io//MS-AMP/docs/getting-started/run-msamp"},"here"),".\nFor more comprehensive examples, please go to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Azure/MS-AMP-Examples"},"MS-AMP-Examples"),"."))}m.isMDXComponent=!0}}]);
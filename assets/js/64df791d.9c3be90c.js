"use strict";(self.webpackChunkmsamp_website=self.webpackChunkmsamp_website||[]).push([[498],{7620:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return m}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),i=["components"],s={id:"usage"},l="Use MS-AMP",p={unversionedId:"user-tutorial/usage",id:"user-tutorial/usage",isDocsHomePage:!1,title:"Use MS-AMP",description:"Enabling MS-AMP is very simple when traning model on single GPU, you only need to add one line of code msamp.initialize(model, optimizer, opt_level) after defining model and optimizer.",source:"@site/../docs/user-tutorial/usage.md",sourceDirName:"user-tutorial",slug:"/user-tutorial/usage",permalink:"/MS-AMP/docs/user-tutorial/usage",editUrl:"https://github.com/azure/MS-AMP/edit/main/website/../docs/user-tutorial/usage.md",version:"current",frontMatter:{id:"usage"},sidebar:"docs",previous:{title:"Run examples",permalink:"/MS-AMP/docs/getting-started/run-msamp"},next:{title:"Container Images",permalink:"/MS-AMP/docs/user-tutorial/container-images"}},m=[],d={toc:m};function u(e){var t=e.components,a=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"use-ms-amp"},"Use MS-AMP"),(0,o.kt)("p",null,"Enabling MS-AMP is very simple when traning model on single GPU, you only need to add one line of code ",(0,o.kt)("inlineCode",{parentName:"p"},"msamp.initialize(model, optimizer, opt_level)")," after defining model and optimizer."),(0,o.kt)("p",null,"Example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import msamp\n\n# Declare model and optimizer as usual, with default (FP32) precision\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n# Allow MS-AMP to perform casts as required by the opt_level\nmodel, optimizer = msamp.initialize(model, optimizer, opt_level="O2")\n...\n')),(0,o.kt)("p",null,"For distributed training job, you need to add ",(0,o.kt)("inlineCode",{parentName:"p"},"optimizer.all_reduce_grads(model)")," after backward to reduce gradients in process group."),(0,o.kt)("p",null,"Example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"scaler = torch.cuda.amp.GradScaler()\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data, target = data.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model(data)\n    loss = loss(output, target)\n    scaler.scale(loss).backward()\n    optimizer.all_reduce_grads(model)\n    scaler.step(optimizer)\n")),(0,o.kt)("p",null,'For applying MS-AMP to DeepSpeed ZeRO, add a "msamp" section in deepspeed config file:'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'"msamp": {\n  "enabled": true,\n  "opt_level": "O3"\n}\n')),(0,o.kt)("p",null,"Runnable, comprehensive examples demonstrating good practices can be found ",(0,o.kt)("a",{parentName:"p",href:"https://azure.github.io//MS-AMP/docs/getting-started/run-msamp"},"here"),".\nFor more examples, please go to ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Azure/MS-AMP-Examples"},"MS-AMP-Examples"),"."))}u.isMDXComponent=!0}}]);